AWSTemplateFormatVersion: 2010-09-09
Description: An S3 Batch Solution for Copying above 5GB S3 Object Size.
Metadata:
  Version: '1.0.1'
  License:
    Description: >-
      'Copyright 2017 Amazon.com, Inc. and its affiliates. All Rights Reserved.
      Licensed under the Amazon Software License (the "License"). You may not
      use this file except in compliance with the License. A copy of the License
      is located at

      http://aws.amazon.com/asl/

      or in the "license" file accompanying this file. This file is distributed
      on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
      express or implied. See the License for the specific language governing
      permissions and limitations under the License.'

  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "Source Bucket, Destination Bucket and Prefix Parameters"
        Parameters:
          - SourceBucket
          - BucketForCopyDestination
          - BucketForCopyDestinationPrefix
      -
        Label:
          default: "S3 Batch Operations Manifest and Report Destination Parameters"
        Parameters:
          - BucketForManifestOrInventory
          - BucketForBatchOpsReport
      -
        Label:
          default: "AWS SDK Performance, Chunksize and Retry Parameters"
        Parameters:
          - TransferMaximumConcurrency
          - SDKMaxPoolConnections
          - SDKMaxErrorRetries
          - MultipartChunkSize
          - MultipartThreshold          
      -
        Label:
          default: "Copy Parameters"
        Parameters:
          - CopyMetadata
          - CopyTagging
          - StorageClass
          - UpdateKeyLeadingPrefix

    ParameterLabels:
      BucketForCopyDestination:
        default: "Destination Bucket"
      BucketForCopyDestinationPrefix:
        default: "Destination Bucket Prefix"
      BucketForManifestOrInventory:
        default: "Bucket where Manifest is Located"
      BucketForBatchOpsReport:
        default: "Bucket where Batch Operations Report is Delivered"


Parameters:
  SourceBucket:
    Description: Please enter the Source Amazon S3 Bucket Name
    Type: String
    MinLength: '3'
    MaxLength: '63'
  BucketForCopyDestination:
    Description: Please enter the Copy Destination S3 Bucket Name
    Type: String
    MinLength: '3'
    MaxLength: '63'
  BucketForCopyDestinationPrefix:
    Description: Please specify the Destination Bucket Prefix or Path without the starting or trailing "/", Leave Blank to Retain Original Path
    Type: String
    MinLength: '0'
    MaxLength: '1024'
  BucketForManifestOrInventory:
    Description: Please enter the S3 Bucket name where the CSV manifest or S3 Inventory is located
    Type: String
    MinLength: '3'
    MaxLength: '63'
  BucketForBatchOpsReport:
    Description: Please enter the S3 Bucket name you want Batch Operation Job Report Delivered to
    Type: String
    MinLength: '3'
    MaxLength: '63'
  TransferMaximumConcurrency:
    Description: The maximum number of concurrent requests SDK uses
    Type: Number
    MinValue: 10
    MaxValue: 940    
    Default: 940
    ConstraintDescription: Concurrent Requests Value must be a Valid Integer between 10 to 940!
  SDKMaxPoolConnections:
    Description: The maximum number of connections SDK keeps in a connection pool
    Type: Number
    MinValue: 10
    MaxValue: 940    
    Default: 940
    ConstraintDescription: Concurrent Requests Value must be a Valid Integer between 10 to 940!
  SDKMaxErrorRetries:
    Description: The number of SDK error retries
    Type: Number
    MinValue: 100
    MaxValue: 100    
    Default: 100
    ConstraintDescription: SDK Retries Value must be a Valid Integer and not lower than 100!
  MultipartChunkSize:
    Description: Multipart Chunk size in bytes (MB*1024*1024) that the SDK uses for multipart transfers.
    Type: String
    Default: 16777216
    MinLength: '1'
    MaxLength: '10'
    AllowedPattern: '[1-9][0-9]*'
    ConstraintDescription: Multipart Chunk size Value must be a Valid Integer!
  MultipartThreshold:
    Description: Multipart Threshold size in bytes (MB*1024*1024) when the SDK switches to multipart transfers.
    Type: Number
    Default: 5368709120
    MinValue: 8388608
    MaxValue: 5368709120
    AllowedValues:
      - 8388608
      - 16777216
      - 104857600      
      - 5368709120 
    ConstraintDescription: Multipart Threshold size Value must be a Valid Integer!
  CopyMetadata:
    AllowedValues:
      - Enable  # Copies metadata
      - Disable  # Do not copy metadata
    Description: Please Choose to enable or disable copying source object metadata to destination
    Type: String
  CopyTagging:
    AllowedValues:
      - Enable  # Copies Tags
      - Disable  # Do not copy Tags
    Description: Please Choose to enable or disable copying source object tags to destination
    Type: String
  StorageClass:
    AllowedValues:
      - STANDARD
      - STANDARD_IA
      - ONEZONE_IA
      - INTELLIGENT_TIERING
      - GLACIER_IR
      - GLACIER
      - DEEP_ARCHIVE
    Description: Please Choose your desired S3 Storage-Class for the Object Copy
    Default: STANDARD
    Type: String

  CopyFunctionReservedConcurrency:
    Description: Choose Unreserved to allow S3 Batch utilize up to 1,000 concurrency, or optionally specify the reserved concurrency for the Lambda function S3 Batch Operations Invokes to perform Copy operations. Note, setting a value impacts the concurrency pool available to other functions. 
    Type: String
    Default: Unreserved
    ConstraintDescription: Value must be within the range of 0 to 200
    AllowedValues:
      - Unreserved
      - 200
      - 150
      - 100
      - 50
      - 10

  UpdateKeyLeadingPrefix:
    AllowedValues:
      - Enable
      - Disable
    Description: Choose Yes to Update the leading Prefix to the New Destination Prefix. For Example oldprefix/myobject becomes newprefix/myobject instead of default oldprefix/newprefix/myobject
    Default: Disable
    Type: String

   


Conditions:
     NoFunctionConcurrency: !Equals [!Ref CopyFunctionReservedConcurrency, Unreserved]     


Resources:

  S3BatchCopyLambdaFunctionIamRole:
    Type: 'AWS::IAM::Role'
    Properties:
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: S3BatchCopyLambdaFunctionIamRolePolicy0
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - 's3:GetObject'
                  - 's3:GetObjectAcl'
                  - 's3:GetObjectTagging'
                  - 's3:GetObjectVersion'
                  - 's3:GetObjectVersionAcl'
                  - 's3:GetObjectVersionTagging'
                  - 's3:ListBucket*'
                Resource:
                  - !Sub arn:${AWS::Partition}:s3:::${SourceBucket}
                  - !Sub arn:${AWS::Partition}:s3:::${SourceBucket}/*                
                Effect: Allow
              - Action:
                  - 's3:PutObject'
                  - 's3:PutObjectAcl'
                  - 's3:PutObjectTagging'
                  - 's3:PutObjectLegalHold'
                  - 's3:PutObjectRetention'
                  - 's3:GetBucketObjectLockConfiguration'
                  - 's3:ListBucket*'
                  - 's3:GetBucketLocation'
                Resource:
                  - !Sub arn:${AWS::Partition}:s3:::${BucketForCopyDestination}
                  - !Sub arn:${AWS::Partition}:s3:::${BucketForCopyDestination}/*
                Effect: Allow
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'

  S3BatchCopyLambdafunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      Environment:
        Variables:
          destination_bucket: !Ref BucketForCopyDestination
          destination_bucket_prefix: !Ref BucketForCopyDestinationPrefix
          max_attempts: !Ref SDKMaxErrorRetries
          max_concurrency: !Ref TransferMaximumConcurrency
          max_pool_connections: !Ref SDKMaxPoolConnections
          multipart_chunksize: !Ref MultipartChunkSize
          multipart_threshold: !Ref MultipartThreshold          
          copy_metadata: !Ref CopyMetadata
          copy_tagging: !Ref CopyTagging
          copy_storage_class: !Ref StorageClass
          change_copy_path: !Ref UpdateKeyLeadingPrefix
      Runtime: python3.12
      ReservedConcurrentExecutions: 
        !If [NoFunctionConcurrency, !Ref AWS::NoValue, !Ref CopyFunctionReservedConcurrency ]       
      Timeout: 900
      Description: An S3 Batch Solution for Copying above 5GB S3 Object Size.
      MemorySize: 600
      Handler: index.lambda_handler
      Role: !GetAtt
        - S3BatchCopyLambdaFunctionIamRole
        - Arn
      Code:
        ZipFile: |
            import boto3
            import os
            from urllib import parse
            from botocore.client import Config
            from botocore.exceptions import ClientError as S3ClientError
            from boto3.s3.transfer import TransferConfig
            import logging

            # Define Environmental Variables
            target_bucket = str(os.environ['destination_bucket'])
            my_max_pool_connections = int(os.environ['max_pool_connections'])
            my_max_concurrency = int(os.environ['max_concurrency'])
            my_multipart_chunksize = int(os.environ['multipart_chunksize'])
            my_multipart_threshold = int(os.environ['multipart_threshold'])
            my_max_attempts = int(os.environ['max_attempts'])
            metadata_copy = str(os.environ['copy_metadata'])
            tagging_copy = str(os.environ['copy_tagging'])
            obj_copy_storage_class = str(os.environ['copy_storage_class'])
            new_prefix = str(os.environ['destination_bucket_prefix'])
            replace_leading_prefix = str(os.environ['change_copy_path'])

            # # Set up logging
            logger = logging.getLogger(__name__)
            logger.setLevel('INFO')

            # Enable Verbose logging for Troubleshooting
            # boto3.set_stream_logger("")

            # Set and Declare Configuration Parameters
            transfer_config = TransferConfig(max_concurrency=my_max_concurrency, multipart_chunksize=my_multipart_chunksize, multipart_threshold=my_multipart_threshold)
            config = Config(max_pool_connections=my_max_pool_connections, retries = {'max_attempts': my_max_attempts})
            myargs = {'ACL': 'bucket-owner-full-control', 'StorageClass': obj_copy_storage_class}


            # Instantiate S3Client
            s3Client = boto3.client('s3', config=config)



            def lambda_handler(event, context):
              # Parse job parameters from Amazon S3 batch operations
              jobId = event['job']['id']
              invocationId = event['invocationId']
              invocationSchemaVersion = event['invocationSchemaVersion']

              # Prepare results
              results = []

              # Parse Amazon S3 Key, Key Version, and Bucket ARN
              taskId = event['tasks'][0]['taskId']
              # use unquote_plus to handle various characters in S3 Key name
              s3Key = parse.unquote_plus(event['tasks'][0]['s3Key'], encoding='utf-8')
              s3VersionId = event['tasks'][0]['s3VersionId']
              s3BucketArn = event['tasks'][0]['s3BucketArn']
              s3Bucket = s3BucketArn.split(':')[-1]

              try:
                # Prepare result code and string
                resultCode = None
                resultString = None
                # Remove line feed or carriage return for compatibility with S3 Batch Result Message
                # Will use str.translate to strip '\n' and '\r'. Convert both char to ascii using ord()
                # where '\t' = 9, '\n' = 10 and '\r' = 13
                mycompat = {9: None, 10: None, 13: None}
                # Construct Copy Object
                copy_source = {'Bucket': s3Bucket, 'Key': s3Key}
                # If source key has VersionID, then construct request with VersionID
                if s3VersionId is not None:
                  copy_source['VersionId'] = s3VersionId
                  # Construct/Retrieve get source key metadata
                  if metadata_copy == 'Enable':
                    get_metadata = s3Client.head_object(Bucket=s3Bucket, Key=s3Key, VersionId=s3VersionId)
                  # Construct/Retrieve get source key tagging
                  if tagging_copy == 'Enable':
                    get_obj_tag = s3Client.get_object_tagging(Bucket=s3Bucket, Key=s3Key, VersionId=s3VersionId)
                else:
                  # Construct/Retrieve get source key metadata
                  if metadata_copy == 'Enable':
                    get_metadata = s3Client.head_object(Bucket=s3Bucket, Key=s3Key)
                  # Construct/Retrieve get source key tagging
                  if tagging_copy == 'Enable':
                    get_obj_tag = s3Client.get_object_tagging(Bucket=s3Bucket, Key=s3Key)

                # Construct New Path
                # Construct New Key
                # Allow object prefix/path to be changed
                
                if new_prefix and len(new_prefix) > 0:
                    if replace_leading_prefix == 'Enable':
                        leading_path = s3Key.split('/')[0]
                        newKey = s3Key.replace(leading_path, new_prefix, 1)
                        logger.info(f"Replace Leading Path Activated {newKey}")
                    elif replace_leading_prefix == 'Disable':
                        newKey = "{0}/{1}".format(new_prefix, s3Key)
                    else:
                        logger.error(f"Please select valid values Yes or No for replace leading path")
                else:
                    newKey = s3Key


                newBucket = target_bucket

                # Toggle Metadata or Tagging Copy Based on Enviromental Variables
                # Construct Request Parameters with metadata and tagging from sourceKey
                # Create variables to append as metadata and tagging to destination object
                if metadata_copy == 'Enable':
                  logger.info("Object Metadata Copy Enabled from Source to Destination")
                  cache_control = get_metadata.get('CacheControl')
                  content_disposition = get_metadata.get('ContentDisposition')
                  content_encoding = get_metadata.get('ContentEncoding')
                  content_language = get_metadata.get('ContentLanguage')
                  metadata = get_metadata.get('Metadata')
                  website_redirect_location = get_metadata.get('WebsiteRedirectLocation')
                  expires = get_metadata.get('Expires')
                  # Construct Request With Required and Available Arguments
                  if cache_control:
                    myargs['CacheControl'] = cache_control
                  if content_disposition:
                    myargs['ContentDisposition'] = content_disposition
                  if content_encoding:
                    myargs['ContentEncoding'] = content_encoding
                  if content_language:
                    myargs['ContentLanguage'] = content_language
                  if metadata:
                    myargs['Metadata'] = metadata
                  if website_redirect_location:
                    myargs['WebsiteRedirectLocation'] = website_redirect_location
                  if expires:
                    myargs['Expires'] = expires
                else:
                  logger.info("Object Metadata Copy Disabled")

                if tagging_copy == 'Enable':
                  logger.info("Object Tagging Copy Enabled from Source to Destination")
                  existing_tag_set = (get_obj_tag.get('TagSet'))
                  # Convert the Output from get object tagging to be compatible with transfer s3.copy()
                  tagging_to_s3 = "&".join([f"{parse.quote_plus(d['Key'])}={parse.quote_plus(d['Value'])}" for d in existing_tag_set])
                  # Construct Request With Required and Available Arguments
                  if existing_tag_set:
                    myargs['Tagging'] = tagging_to_s3
                else:
                  logger.info("Object Tagging Copy Disabled")

                # Initiate the Actual Copy Operation and include transfer config option
                logger.info(f"starting copy of object {s3Key} with versionID {s3VersionId} between SOURCEBUCKET: {s3Bucket} and DESTINATIONBUCKET: {newBucket}")
                response = s3Client.copy(copy_source, newBucket, newKey, Config=transfer_config, ExtraArgs=myargs)
                # Confirm copy was successful
                logger.info("Successfully completed the copy process!")

                # Mark as succeeded
                resultCode = 'Succeeded'
                resultString = str(response)
              except S3ClientError as e:
                # log errors, some errors does not have a response, so handle them
                logger.error(f"Unable to complete requested operation, see Clienterror details below:")
                try:
                  logger.error(e.response)
                  errorCode = e.response.get('Error', {}).get('Code')
                  errorMessage = e.response.get('Error', {}).get('Message')
                  errorS3RequestID = e.response.get('ResponseMetadata', {}).get('RequestId')
                  errorS3ExtendedRequestID = e.response.get('ResponseMetadata', {}).get('HostId')
                  resultCode = 'PermanentFailure'
                  resultString = '{}: {}: {}: {}'.format(errorCode, errorMessage, errorS3RequestID, errorS3ExtendedRequestID)
                except AttributeError:
                  logger.error(e)
                  resultCode = 'PermanentFailure'
                  # Remove line feed or carriage return for compatibility with S3 Batch Result Message
                  resultString = '{}'.format(str(e).translate(mycompat))
              except Exception as e:
                # log errors, some errors does not have a response, so handle them
                logger.error(f"Unable to complete requested operation, see Additional Client/Service error details below:")
                try:
                  logger.error(e.response)
                  errorCode = e.response.get('Error', {}).get('Code')
                  errorMessage = e.response.get('Error', {}).get('Message')
                  errorS3RequestID = e.response.get('ResponseMetadata', {}).get('RequestId')
                  errorS3ExtendedRequestID = e.response.get('ResponseMetadata', {}).get('HostId')
                  resultString = '{}: {}: {}: {}'.format(errorCode, errorMessage, errorS3RequestID, errorS3ExtendedRequestID)
                except AttributeError:
                  logger.error(e)
                  resultString = 'Exception: {}'.format(str(e).translate(mycompat))
                resultCode = 'PermanentFailure'

              finally:
                results.append({
                'taskId': taskId,
                'resultCode': resultCode,
                'resultString': resultString
                })

              return {
              'invocationSchemaVersion': invocationSchemaVersion,
              'treatMissingKeysAs': 'PermanentFailure',
              'invocationId': invocationId,
              'results': results
              }

############################ End Code ########################################
  S3BatchOperationsServiceIamRole:
    Type: 'AWS::IAM::Role'
    Properties:
      Policies:
        - PolicyName: AllowManifestGeneration
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - 's3:GetBucketLocation'
                  - 's3:PutInventoryConfiguration'
                Resource:
                  - !Sub arn:${AWS::Partition}:s3:::${SourceBucket}              
                Effect: Allow      
        - PolicyName: S3BatchOperationsServiceIamRolePolicy0
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - 's3:GetObject'
                  - 's3:GetObjectVersion'
                  - 's3:GetBucketLocation'
                Resource:
                  - !Sub arn:${AWS::Partition}:s3:::${BucketForManifestOrInventory}
                  - !Sub arn:${AWS::Partition}:s3:::${BucketForManifestOrInventory}/*
                Effect: Allow
              - Action:
                  - 's3:PutObject'
                  - 's3:GetBucketLocation'
                Resource:
                  - !Sub arn:${AWS::Partition}:s3:::${BucketForBatchOpsReport}
                  - !Sub arn:${AWS::Partition}:s3:::${BucketForBatchOpsReport}/*
                Effect: Allow
              - Action:
                  - 'lambda:InvokeFunction'
                Resource: !Sub 'arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:${S3BatchCopyLambdafunction}*'
                Effect: Allow
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: batchoperations.s3.amazonaws.com
            Action: 'sts:AssumeRole'


Outputs:
  SourceS3Bucket:
    Description: The Source Amazon S3 Bucket where the solution will copy the objects from
    Value: !Sub ${SourceBucket}
  DestinationS3Bucket:
    Description: The S3 Bucket where the solution will copy the objects to
    Value: !Sub ${BucketForCopyDestination}
  DestinationS3BucketPrefix:
    Description: The Prefix in the Destination S3 Bucket where the solution will copy the objects to
    Value: !Sub ${BucketForCopyDestinationPrefix}
  LambdaFunctionIAMRoleforCopy:
    Value: !Ref S3BatchCopyLambdaFunctionIamRole
    Description: IAM Role associated with the Lambda function invoked by Batch Operations for the copy operation. This might require applicable KMS key permissions. 
  LambdaFunctionforCopyARN:
    Value: !GetAtt S3BatchCopyLambdafunction.Arn
    Description: ARN of the Lambda function invoked by Batch Operations for the copy operation.  
  ManifestOrInventorySourceBucket:
    Description: The S3 Bucket where the CSV manifest or Inventory is located
    Value: !Sub ${BucketForManifestOrInventory}
  BatchOperationsReportDestination:
    Description: The S3 Bucket where S3 Batch Operation Job Report is delivered
    Value: !Sub ${BucketForBatchOpsReport}
  SDKRetries:
    Description: The number of times SDK will retry a failed request
    Value: !Sub ${SDKMaxErrorRetries}
  MaxConcurency:
    Description: The number of concurrent threads the SDK uses
    Value: !Sub ${TransferMaximumConcurrency}
  MaxPoolConnection:
    Description: The maximum number of HTTP/TCP connections pools SDK uses
    Value: !Sub ${SDKMaxPoolConnections}
  ObjectMultiPartChunkSize:
    Description: This is the chunk size that the SDK uses for multipart transfers of individual files
    Value: !Sub ${MultipartChunkSize}
  ObjectMultiPartThreshold:
    Description: This is the chunk size that the SDK uses for multipart transfers of individual files
    Value: !Sub ${MultipartThreshold}    
